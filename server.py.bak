"""
PROJECT: PersonaPlex Backend Node (Single-File Prototype)
DATE: 2026-01-27
CONTEXT:
This is the high-performance edge node that runs the NVIDIA PersonaPlex 7B model.
It acts as a WebSocket bridge between the React Frontend and the PyTorch Model.

ARCHITECTURE:
1. Framework: FastAPI (Async Python is required for handling concurrent streams).
2. Protocol: WebSockets (Binary frames for audio, Text frames for config).
3. Audio Spec: 24kHz, Mono, Float32 (Standard for Moshi/PersonaPlex).

REFACTORING GOAL:
Break this into a production-grade Python package:
- /app/main.py (FastAPI entry point)
- /app/services/model_wrapper.py (The GPU Inference Class)
- /app/core/audio_utils.py (PCM conversion logic)
- /app/routers/websocket.py (Connection handling)
"""

import asyncio
import json
import logging
import struct
import time
from typing import List

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
import numpy as np

# Configure Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("PersonaPlex-Node")

app = FastAPI(title="PersonaPlex Edge Node")

# --- CONSTANTS ---
SAMPLE_RATE = 24000
CHUNK_SIZE = 512  # Audio chunk size in frames

# --- MOCK MODEL WRAPPER ---
# [REFACTOR NOTE]
# In the real implementation, this class will import `torch` and load the 
# 7B parameter model into VRAM.
class PersonaPlexEngine:
    def __init__(self):
        self.persona = "Default Assistant"
        self.voice_id = "natural_female_1"
        self.history = []
        logger.info("Initializing PersonaPlex Engine (Mock)...")
        # time.sleep(2) # Simulate model loading time
        logger.info("Engine Ready.")

    def configure(self, persona: str, voice_id: str):
        """Updates the system prompt and voice token embedding."""
        self.persona = persona
        self.voice_id = voice_id
        logger.info(f"Context updated: {persona[:30]}... | Voice: {voice_id}")

    def process_audio_frame(self, audio_bytes: bytes) -> bytes:
        """
        CORE INFERENCE LOOP
        Input: Raw PCM bytes from user.
        Output: Raw PCM bytes from AI.
        
        Real Workflow:
        1. Decode bytes -> Float32 Tensor.
        2. model.encode(user_audio) -> tokens.
        3. model.generate_step(tokens) -> predicted_tokens.
        4. model.decode(predicted_tokens) -> Float32 Audio.
        5. Encode Float32 -> bytes.
        """
        
        # [SIMULATION]
        # We just generate noise here to prove the pipe is open.
        # In production, this block is replaced by: 
        # return self.model.step(audio_bytes)
        
        # Simulate processing time (latency)
        # time.sleep(0.005) 
        
        # Generate random noise (static) as a placeholder response
        # Using numpy to generate Float32 samples, then converting to bytes
        noise = np.random.uniform(-0.1, 0.1, CHUNK_SIZE).astype(np.float32)
        return noise.tobytes()

# Global Model Instance
# In a real app, manage this via dependency injection or a singleton pattern
engine = PersonaPlexEngine()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    Main Duplex Loop.
    Maintains the connection alive while simultaneously reading user audio
    and writing AI audio.
    """
    await websocket.accept()
    logger.info("Client Connected via WebSocket")
    
    try:
        while True:
            # 1. AWAIT INPUT
            # We wait for a message from the client (Frontend).
            # This can be text (config) or bytes (audio).
            message = await websocket.receive()
            
            # 2. HANDLE CONFIGURATION
            if "text" in message:
                try:
                    config = json.loads(message["text"])
                    if config.get("type") == "config":
                        engine.configure(
                            config.get("persona", ""),
                            config.get("voice", "")
                        )
                except json.JSONDecodeError:
                    logger.error("Failed to parse config JSON")

            # 3. HANDLE AUDIO STREAM (HOT PATH)
            elif "bytes" in message:
                user_audio_chunk = message["bytes"]
                
                # --- INFERENCE STEP ---
                # Pass the user's voice to the model
                ai_audio_chunk = engine.process_audio_frame(user_audio_chunk)
                
                # --- RESPONSE STEP ---
                # Send the AI's voice back to the client immediately
                await websocket.send_bytes(ai_audio_chunk)

    except WebSocketDisconnect:
        logger.info("Client Disconnected")
    except Exception as e:
        logger.error(f"Unexpected Error: {e}")
        # Optionally send error back to client before closing
        # await websocket.send_json({"error": str(e)})

if __name__ == "__main__":
    # HOSTING NOTE:
    # 0.0.0.0 allows external access (e.g. from a different device on LAN).
    # Port 8000 is the default.
    uvicorn.run(app, host="0.0.0.0", port=8000)